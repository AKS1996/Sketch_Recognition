{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle CSV\n",
    "#### Do not run if you already have the shuffled training files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import json\n",
    "import os\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f2cat(filename: str) -> str:\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "class Simplified():\n",
    "    def __init__(self, input_path='/home/k.vincent/apm_file/'):\n",
    "        self.input_path = input_path\n",
    "\n",
    "    def list_all_categories(self):\n",
    "        files = os.listdir(os.path.join(self.input_path, 'train_simplified'))\n",
    "        return sorted([f2cat(f) for f in files], key=str.lower)\n",
    "\n",
    "    def read_training_csv(self, category, nrows=None, usecols=None, drawing_transform=False):\n",
    "        df = pd.read_csv(os.path.join(self.input_path, 'train_simplified', category + '.csv'),\n",
    "                         nrows=nrows, parse_dates=['timestamp'], usecols=usecols)\n",
    "        if drawing_transform:\n",
    "            df['drawing'] = df['drawing'].apply(json.loads)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "s = Simplified('/home/k.vincent/apm_file/')\n",
    "NCSVS = 100\n",
    "categories = s.list_all_categories()\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [23:06,  4.55s/it]\n"
     ]
    }
   ],
   "source": [
    "for y, cat in tqdm(enumerate(categories)):\n",
    "    df = s.read_training_csv(cat, nrows=None)\n",
    "    df['y'] = y\n",
    "    df['cv'] = (df.key_id // 10 ** 7) % NCSVS\n",
    "    for k in range(NCSVS):\n",
    "        filename = 'train_k{}.csv'.format(k)\n",
    "        chunk = df[df.cv == k]\n",
    "        chunk = chunk.drop(['key_id'], axis=1)\n",
    "        if y == 0:\n",
    "            chunk.to_csv(filename, index=False)\n",
    "        else:\n",
    "            chunk.to_csv(filename, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [4:48:30<00:00, 173.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497740, 7)\n",
      "Latest run 2018-12-02 04:30:47.906755.\n",
      "Total time 18703s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(NCSVS)):\n",
    "    filename = 'train_k{}.csv'.format(k)\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['rnd'] = np.random.rand(len(df))\n",
    "        df = df.sort_values(by='rnd').drop('rnd', axis=1)\n",
    "        df.to_csv(filename + '.gz', compression='gzip', index=False)\n",
    "        os.remove(filename)\n",
    "print(df.shape)\n",
    "end = dt.datetime.now()\n",
    "print('Latest run {}.\\nTotal time {}s'.format(end, (end - start).seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import ast\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "start = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DP_DIR = '/home/k.vincent/'\n",
    "INPUT_DIR = '/home/k.vincent/apm_file/'\n",
    "BASE_SIZE = 256\n",
    "NCSVS = 100\n",
    "NCATS = 340\n",
    "np.random.seed(seed=1987)\n",
    "tf.set_random_seed(seed=1987)\n",
    "def f2cat(filename: str) -> str:\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "def list_all_categories():\n",
    "    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n",
    "    return sorted([f2cat(f) for f in files], key=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=3):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=3):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "\n",
    "def preds2catids(predictions):\n",
    "    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_single_cnn(size, conv_layers=(8, 16, 32, 64), dense_layers=(512, 256), conv_dropout=0.2,\n",
    "                      dense_dropout=0.2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add( Conv2D(conv_layers[0], kernel_size=(3, 3), padding='same', activation='relu', input_shape=(size, size, 3)) )\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    \n",
    "    for conv_layer_size in conv_layers[1:]:\n",
    "        model.add(Conv2D(conv_layer_size, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        if conv_dropout:\n",
    "            model.add(Dropout(conv_dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    if dense_dropout:\n",
    "        model.add(Dropout(dense_dropout))\n",
    "\n",
    "    for dense_layer_size in dense_layers:\n",
    "        model.add(Dense(dense_layer_size, activation='relu'))\n",
    "        model.add(Activation('relu'))\n",
    "        if dense_dropout:\n",
    "            model.add(Dropout(dense_dropout))\n",
    "\n",
    "    model.add(Dense(NCATS, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "def top3_acc( tgt, pred ):\n",
    "    sc = np.mean( (pred[:,0]==tgt) | (pred[:,1]==tgt) | (pred[:,2]==tgt) )\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 82, 82, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 39, 39, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 17, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              33556480  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 340)               696660    \n",
      "=================================================================\n",
      "Total params: 34,699,476\n",
      "Trainable params: 34,699,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "bi_factor = 4\n",
    "STEPS = 500*bi_factor\n",
    "size = 82\n",
    "batchsize = 2048/bi_factor\n",
    "\n",
    "model = custom_single_cnn(size=size,\n",
    "                          conv_layers=[128, 128, 256],\n",
    "                          dense_layers=[2048],\n",
    "                          conv_dropout=False,\n",
    "                          dense_dropout=0.10 )\n",
    "model.compile(optimizer=Adam(lr=0.002), loss='categorical_crossentropy',\n",
    "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = [(255, 0, 0) , (255, 255, 0),  (128, 255, 0),  (0, 255, 0), (0, 255, 128), (0, 255, 255), \n",
    "          (0, 128, 255), (0, 0, 255), (128, 0, 255), (255, 0, 255)]\n",
    "def draw_cv2(raw_strokes, size=256, lw=6):\n",
    "    img = np.zeros((BASE_SIZE, BASE_SIZE,3), np.uint8)\n",
    "    for t, stroke in enumerate(raw_strokes):\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            color = colors[min(t, len(colors)-1)]\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]),/\n",
    "                         color, lw, lineType=cv2.LINE_AA)\n",
    "    if np.random.rand()>0.5:\n",
    "        img = np.fliplr(img)\n",
    "    if np.random.rand()>0.75:\n",
    "        if np.random.rand()>0.50:\n",
    "            img = img[ 4:, 4: ,:]\n",
    "        else:\n",
    "            img = img[ :-4, :-4 ,:]\n",
    "    if np.random.rand()>0.50:\n",
    "        img2 = cv2.resize(img, (200, 200))\n",
    "        img = np.zeros((BASE_SIZE, BASE_SIZE,3), np.uint8)\n",
    "        img[18:218,18:218, :] = img2\n",
    "\n",
    "    if size != BASE_SIZE:\n",
    "        return cv2.resize(img, (size, size))\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def image_generator(size, batchsize, ks, lw=6):\n",
    "    while True:\n",
    "        for k in np.random.permutation(ks):\n",
    "            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n",
    "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "                df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "                x = np.zeros((len(df), size, size,3))\n",
    "                for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                    #print(df.drawing.values)\n",
    "                    #\n",
    "                    x[i, :, :, :] = draw_cv2(raw_strokes, size=size, lw=lw)\n",
    "                x = x / 255.\n",
    "                x = x.reshape((len(df), size, size, 3)).astype(np.float32)\n",
    "                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n",
    "                yield x, y\n",
    "\n",
    "def df_to_image_array(df, size, lw=6):\n",
    "    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "    x = np.zeros((len(df), size, size,3))\n",
    "    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "        #\n",
    "        x[i, :, : ,:] = draw_cv2(raw_strokes, size=size, lw=lw)\n",
    "    x = x / 255.\n",
    "    x = x.reshape((len(df), size, size, 3)).astype(np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 82, 82, 3) (30000, 340)\n",
      "Validation array memory 2.25 GB\n"
     ]
    }
   ],
   "source": [
    "valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(NCSVS - 1)), nrows=30000)\n",
    "x_valid = df_to_image_array(valid_df, size)\n",
    "y_valid = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print('Validation array memory {:.2f} GB'.format(x_valid.nbytes / 1024.**3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = image_generator(size=size, batchsize=batchsize, ks=range(NCSVS - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model was trained by 2000 steps a epochs, 10 epochs a time. The model was trained for 10 additional epochs before the below training started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 2.2206 - categorical_crossentropy: 2.2206 - categorical_accuracy: 0.4940 - top_3_accuracy: 0.6840\n",
      "Epoch 00001: val_top_3_accuracy improved from -inf to 0.79060, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1651s 825ms/step - loss: 2.2203 - categorical_crossentropy: 2.2203 - categorical_accuracy: 0.4941 - top_3_accuracy: 0.6841 - val_loss: 1.6645 - val_categorical_crossentropy: 1.6645 - val_categorical_accuracy: 0.6005 - val_top_3_accuracy: 0.7906\n",
      "Epoch 2/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.6443 - categorical_crossentropy: 1.6443 - categorical_accuracy: 0.6054 - top_3_accuracy: 0.7919\n",
      "Epoch 00002: val_top_3_accuracy improved from 0.79060 to 0.81837, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1613s 807ms/step - loss: 1.6443 - categorical_crossentropy: 1.6443 - categorical_accuracy: 0.6054 - top_3_accuracy: 0.7919 - val_loss: 1.4880 - val_categorical_crossentropy: 1.4880 - val_categorical_accuracy: 0.6414 - val_top_3_accuracy: 0.8184\n",
      "Epoch 3/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.5331 - categorical_crossentropy: 1.5331 - categorical_accuracy: 0.6285 - top_3_accuracy: 0.8103\n",
      "Epoch 00003: val_top_3_accuracy improved from 0.81837 to 0.82953, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1625s 813ms/step - loss: 1.5331 - categorical_crossentropy: 1.5331 - categorical_accuracy: 0.6285 - top_3_accuracy: 0.8103 - val_loss: 1.4261 - val_categorical_crossentropy: 1.4261 - val_categorical_accuracy: 0.6532 - val_top_3_accuracy: 0.8295\n",
      "Epoch 4/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.4779 - categorical_crossentropy: 1.4779 - categorical_accuracy: 0.6415 - top_3_accuracy: 0.8199\n",
      "Epoch 00004: val_top_3_accuracy improved from 0.82953 to 0.83480, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1643s 821ms/step - loss: 1.4780 - categorical_crossentropy: 1.4780 - categorical_accuracy: 0.6414 - top_3_accuracy: 0.8199 - val_loss: 1.3766 - val_categorical_crossentropy: 1.3766 - val_categorical_accuracy: 0.6670 - val_top_3_accuracy: 0.8348\n",
      "Epoch 5/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.4424 - categorical_crossentropy: 1.4424 - categorical_accuracy: 0.6490 - top_3_accuracy: 0.8253\n",
      "Epoch 00005: val_top_3_accuracy improved from 0.83480 to 0.84077, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1634s 817ms/step - loss: 1.4424 - categorical_crossentropy: 1.4424 - categorical_accuracy: 0.6491 - top_3_accuracy: 0.8253 - val_loss: 1.3501 - val_categorical_crossentropy: 1.3501 - val_categorical_accuracy: 0.6710 - val_top_3_accuracy: 0.8408\n",
      "Epoch 6/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.4166 - categorical_crossentropy: 1.4166 - categorical_accuracy: 0.6544 - top_3_accuracy: 0.8295\n",
      "Epoch 00006: val_top_3_accuracy improved from 0.84077 to 0.84730, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1638s 819ms/step - loss: 1.4166 - categorical_crossentropy: 1.4166 - categorical_accuracy: 0.6544 - top_3_accuracy: 0.8295 - val_loss: 1.3160 - val_categorical_crossentropy: 1.3160 - val_categorical_accuracy: 0.6785 - val_top_3_accuracy: 0.8473\n",
      "Epoch 7/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.3957 - categorical_crossentropy: 1.3957 - categorical_accuracy: 0.6593 - top_3_accuracy: 0.8328\n",
      "Epoch 00007: val_top_3_accuracy did not improve from 0.84730\n",
      "2000/2000 [==============================] - 1639s 819ms/step - loss: 1.3958 - categorical_crossentropy: 1.3958 - categorical_accuracy: 0.6593 - top_3_accuracy: 0.8328 - val_loss: 1.3046 - val_categorical_crossentropy: 1.3046 - val_categorical_accuracy: 0.6813 - val_top_3_accuracy: 0.8464\n",
      "Epoch 8/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.3805 - categorical_crossentropy: 1.3805 - categorical_accuracy: 0.6635 - top_3_accuracy: 0.8360\n",
      "Epoch 00008: val_top_3_accuracy improved from 0.84730 to 0.84870, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1638s 819ms/step - loss: 1.3805 - categorical_crossentropy: 1.3805 - categorical_accuracy: 0.6635 - top_3_accuracy: 0.8360 - val_loss: 1.2972 - val_categorical_crossentropy: 1.2972 - val_categorical_accuracy: 0.6807 - val_top_3_accuracy: 0.8487\n",
      "Epoch 9/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.3666 - categorical_crossentropy: 1.3666 - categorical_accuracy: 0.6663 - top_3_accuracy: 0.8380\n",
      "Epoch 00009: val_top_3_accuracy improved from 0.84870 to 0.85303, saving model to ./black-white-7.model\n",
      "2000/2000 [==============================] - 1640s 820ms/step - loss: 1.3665 - categorical_crossentropy: 1.3665 - categorical_accuracy: 0.6663 - top_3_accuracy: 0.8380 - val_loss: 1.2890 - val_categorical_crossentropy: 1.2890 - val_categorical_accuracy: 0.6855 - val_top_3_accuracy: 0.8530\n",
      "Epoch 10/10\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.3581 - categorical_crossentropy: 1.3581 - categorical_accuracy: 0.6678 - top_3_accuracy: 0.8392\n",
      "Epoch 00010: val_top_3_accuracy did not improve from 0.85303\n",
      "2000/2000 [==============================] - 1626s 813ms/step - loss: 1.3581 - categorical_crossentropy: 1.3581 - categorical_accuracy: 0.6678 - top_3_accuracy: 0.8392 - val_loss: 1.2716 - val_categorical_crossentropy: 1.2716 - val_categorical_accuracy: 0.6893 - val_top_3_accuracy: 0.8523\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    #EarlyStopping(monitor='val_top_3_accuracy', patience=15, min_delta=0.001, mode='max'),\n",
    "    ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=5, min_delta=0.005, mode='max', cooldown=3),\n",
    "    ModelCheckpoint(\"./RGBsim-1.model\",monitor='val_top_3_accuracy', mode = 'max', save_best_only=True, verbose=1)\n",
    "]\n",
    "hist = model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=10, verbose=1,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks = callbacks\n",
    ")\n",
    "\n",
    "model.save('Autosaved_CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validate the accuracy and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 19s 617us/step\n",
      "Map3: 0.761\n",
      "Top3: 0.852\n",
      "\n",
      "30000/30000 [==============================] - 18s 588us/step\n",
      "Map3: 0.746\n",
      "Top3: 0.843\n",
      "\n",
      "Map3: 0.772\n",
      "Top3: 0.861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy? \n",
    "valid_predictions1 = model.predict(x_valid, batch_size=128, verbose=1)\n",
    "map3 = mapk(valid_df[['y']].values, preds2catids(valid_predictions1).values)\n",
    "top3 = top3_acc(valid_df[['y']].values.flatten(), preds2catids(valid_predictions1).values)\n",
    "print('Map3: {:.3f}'.format(map3))\n",
    "print('Top3: {:.3f}'.format(top3))\n",
    "print()\n",
    "\n",
    "x_valid2 = np.array( [ np.fliplr(x_valid[i]) for i in range(x_valid.shape[0])] )\n",
    "valid_predictions2 = model.predict(x_valid2, batch_size=128, verbose=1)\n",
    "map3 = mapk(valid_df[['y']].values, preds2catids(valid_predictions2).values)\n",
    "top3 = top3_acc(valid_df[['y']].values.flatten(), preds2catids(valid_predictions2).values)\n",
    "print('Map3: {:.3f}'.format(map3))\n",
    "print('Top3: {:.3f}'.format(top3))\n",
    "print()\n",
    "\n",
    "map3 = mapk(valid_df[['y']].values, preds2catids(0.5*valid_predictions1+0.5*valid_predictions2).values)\n",
    "top3 = top3_acc(valid_df[['y']].values.flatten(), preds2catids(0.5*valid_predictions1+0.5*valid_predictions2).values)\n",
    "print('Map3: {:.3f}'.format(map3))\n",
    "print('Top3: {:.3f}'.format(top3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the file if you get OOM exception\n",
    "try:\n",
    "    del test\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del x_test\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del x_test2\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del test_predictions1\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del test_predictions2\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del test_predictions\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del top3\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del submission\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del top3cats\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del x_valid2\n",
    "    del valid_predictions1\n",
    "    del valid_predictions2\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del x_valid\n",
    "    del y_valid\n",
    "    del valid_df\n",
    "    del x\n",
    "    pass\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k.vincent/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 5s 633us/step\n",
      "8000/8000 [==============================] - 5s 590us/step\n",
      "8000/8000 [==============================] - 5s 594us/step\n",
      "8000/8000 [==============================] - 5s 589us/step\n",
      "8000/8000 [==============================] - 5s 585us/step\n",
      "8000/8000 [==============================] - 5s 587us/step\n",
      "8000/8000 [==============================] - 5s 600us/step\n",
      "8000/8000 [==============================] - 5s 590us/step\n",
      "8000/8000 [==============================] - 5s 585us/step\n",
      "8000/8000 [==============================] - 5s 587us/step\n",
      "8000/8000 [==============================] - 5s 601us/step\n",
      "8000/8000 [==============================] - 5s 591us/step\n",
      "8000/8000 [==============================] - 5s 589us/step\n",
      "8000/8000 [==============================] - 5s 587us/step\n",
      "8000/8000 [==============================] - 5s 599us/step\n",
      "8000/8000 [==============================] - 5s 583us/step\n",
      "8000/8000 [==============================] - 5s 589us/step\n",
      "8000/8000 [==============================] - 5s 585us/step\n",
      "8000/8000 [==============================] - 5s 595us/step\n",
      "8000/8000 [==============================] - 5s 589us/step\n",
      "8000/8000 [==============================] - 5s 592us/step\n",
      "8000/8000 [==============================] - 5s 595us/step\n",
      "8000/8000 [==============================] - 5s 607us/step\n",
      "8000/8000 [==============================] - 5s 596us/step\n",
      "8000/8000 [==============================] - 5s 596us/step\n",
      "8000/8000 [==============================] - 5s 588us/step\n",
      "8000/8000 [==============================] - 5s 597us/step\n",
      "8000/8000 [==============================] - 5s 599us/step\n",
      "199/199 [==============================] - 0s 2ms/step\n",
      "199/199 [==============================] - 0s 617us/step\n"
     ]
    }
   ],
   "source": [
    "#Create Submission\n",
    "#batched prediction due to memory constrain\n",
    "#This is super slow but is memory-friendly\n",
    "INPUT_DIR = '/home/k.vincent/'\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test_simplified.csv'))\n",
    "\n",
    "test_predictions1 = np.zeros(shape=(0,340))\n",
    "test_predictions2 = np.zeros(shape=(0,340))\n",
    "\n",
    "for index, ob in test.groupby(np.arange(len(test))//8000):\n",
    "    x_test = df_to_image_array(ob, size)\n",
    "    x_test2 = np.array( [ np.fliplr(x_test[i]) for i in range(x_test.shape[0])] )\n",
    "    \n",
    "    temp_pred = model.predict(x_test, batch_size=128, verbose=1)\n",
    "    test_predictions1 = np.concatenate((test_predictions1, temp_pred))\n",
    "    \n",
    "    temp_pred = model.predict(x_test2, batch_size=128, verbose=1)\n",
    "    test_predictions2 = np.concatenate((test_predictions2, temp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>radio</td>\n",
       "      <td>stereo</td>\n",
       "      <td>snorkel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hockey_puck</td>\n",
       "      <td>bottlecap</td>\n",
       "      <td>sandwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>castle</td>\n",
       "      <td>The_Great_Wall_of_China</td>\n",
       "      <td>camel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mountain</td>\n",
       "      <td>triangle</td>\n",
       "      <td>tent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>campfire</td>\n",
       "      <td>fireplace</td>\n",
       "      <td>leaf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             a                        b         c\n",
       "0        radio                   stereo   snorkel\n",
       "1  hockey_puck                bottlecap  sandwich\n",
       "2       castle  The_Great_Wall_of_China     camel\n",
       "3     mountain                 triangle      tent\n",
       "4     campfire                fireplace      leaf"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(112199, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_id</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9000003627287624</td>\n",
       "      <td>radio stereo snorkel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9000010688666847</td>\n",
       "      <td>hockey_puck bottlecap sandwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000023642890129</td>\n",
       "      <td>castle The_Great_Wall_of_China camel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9000038588854897</td>\n",
       "      <td>mountain triangle tent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000052667981386</td>\n",
       "      <td>campfire fireplace leaf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             key_id                                  word\n",
       "0  9000003627287624                  radio stereo snorkel\n",
       "1  9000010688666847        hockey_puck bottlecap sandwich\n",
       "2  9000023642890129  castle The_Great_Wall_of_China camel\n",
       "3  9000038588854897                mountain triangle tent\n",
       "4  9000052667981386               campfire fireplace leaf"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(112199, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction ensembling and write file\n",
    "test_predictions = 0.5*test_predictions1 + 0.5*test_predictions2\n",
    "INPUT_DIR = '/home/k.vincent/apm_file/'\n",
    "top3 = preds2catids(test_predictions)\n",
    "cats = list_all_categories()\n",
    "id2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(cats)}\n",
    "top3cats = top3.replace(id2cat)\n",
    "top3cats.head()\n",
    "top3cats.shape\n",
    "\n",
    "test['word'] = top3cats['a'] + ' ' + top3cats['b'] + ' ' + top3cats['c']\n",
    "submission = test[['key_id', 'word']]\n",
    "submission.to_csv('_submit'.format(int(map3 * 10**4)), index=False)\n",
    "submission.head()\n",
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save('fx_CNN')#model = load_model('rdRGB_128to128_50e_CNN', custom_objects={'top_3_accuracy': top_3_accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
